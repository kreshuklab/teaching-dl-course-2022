{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B8zAmXMzQa5"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/constantinpape/dl-teaching-resources/blob/main/exercises/classification/5_data_augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xr3oCM0LzQa8"
   },
   "source": [
    "# Data Augmentation on CIFAR10\n",
    "\n",
    "In this exercise we will use data augmentation to increase the available training data and thus improve the network training performance. We will use the same network architecture as in the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Erdj5852F4mv"
   },
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zyXMvcuCzQbI"
   },
   "outputs": [],
   "source": [
    "# load tensorboard extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iX0lcNv2zQba"
   },
   "outputs": [],
   "source": [
    "# import torch and other libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLbdr5lyzQbk"
   },
   "outputs": [],
   "source": [
    "!pip install cifar2png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_adA5sMRzQb0"
   },
   "outputs": [],
   "source": [
    "# check if we have gpu support\n",
    "# colab offers free gpus, however they are not activated by default.\n",
    "# to activate the gpu, go to 'Runtime->Change runtime type'. \n",
    "# Then select 'GPU' in 'Hardware accelerator' and click 'Save'\n",
    "have_gpu = torch.cuda.is_available()\n",
    "# we need to define the device for torch, yadda yadda\n",
    "if have_gpu:\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"GPU is not available, training will run on the CPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LcngiR2HzQb_"
   },
   "outputs": [],
   "source": [
    "# run this in google colab to get the utils.py file\n",
    "!wget https://raw.githubusercontent.com/constantinpape/training-deep-learning-models-for-vison/master/day1/utils.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UxJnmv9OzQcK"
   },
   "outputs": [],
   "source": [
    "# we will reuse the training function, validation function and\n",
    "# data preparation from the previous notebook\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s790zEKzQcV"
   },
   "outputs": [],
   "source": [
    "cifar_dir = './cifar10'\n",
    "!cifar2png cifar10 cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK8eKTTSzQcb"
   },
   "outputs": [],
   "source": [
    "categories = os.listdir('./cifar10/train')\n",
    "categories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4oLjjV7SGTL"
   },
   "outputs": [],
   "source": [
    "images, labels = utils.load_cifar(os.path.join(cifar_dir, 'train'))\n",
    "(train_images, train_labels,\n",
    " val_images, val_labels) = utils.make_cifar_train_val_split(images, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaWkklrjzQci"
   },
   "source": [
    "## Data Augmentation\n",
    "\n",
    "The goal of data augmentation is to increase the amount of training data by transforming the input images in a way that they still resemble realistic images. Popular transformations used in data augmentation include rotations, image flips, color jitter or additive noise.\n",
    "Here, we will start with two transformations:\n",
    "- random flips along the vertical centerline\n",
    "- random color jitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmCMsZI4NHPE"
   },
   "outputs": [],
   "source": [
    "# define random augmentations\n",
    "import skimage.color as color\n",
    "\n",
    "def random_flip(image, target, probability=.5):\n",
    "    \"\"\" Randomly mirror the image across the vertical axis.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < probability:\n",
    "      image = np.array([np.fliplr(im) for im in image])\n",
    "    return image, target\n",
    "\n",
    "\n",
    "def random_color_jitter(image, target, probability=.5):\n",
    "  \"\"\" Randomly jitter the saturation, hue and brightness of the image.\n",
    "  \"\"\"\n",
    "  if np.random.rand() > probability:\n",
    "    # skimage expects WHC instead of CHW\n",
    "    image = image.transpose((1, 2, 0))\n",
    "    # transform image to hsv color space to apply jitter\n",
    "    image = color.rgb2hsv(image)\n",
    "    # compute jitter factors in range 0.66 - 1.5  \n",
    "    jitter_factors = 1.5 * np.random.rand(3)\n",
    "    jitter_factors = np.clip(jitter_factors, 0.66, 1.5)\n",
    "    # apply the jitter factors, making sure we stay in correct value range\n",
    "    image *= jitter_factors\n",
    "    image = np.clip(image, 0, 1)\n",
    "    # transform back to rgb and CHW\n",
    "    image = color.hsv2rgb(image)\n",
    "    image = image.transpose((2, 0, 1))\n",
    "  return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSPB7ZAjPBkp"
   },
   "outputs": [],
   "source": [
    "# create training dataset with augmentations\n",
    "from functools import partial\n",
    "train_trafos = [\n",
    "    utils.to_channel_first,\n",
    "    utils.normalize,\n",
    "    random_color_jitter,\n",
    "    random_flip,\n",
    "    utils.to_tensor\n",
    "]\n",
    "train_trafos = partial(utils.compose, transforms=train_trafos)\n",
    "\n",
    "train_dataset = utils.DatasetWithTransform(train_images, train_labels,\n",
    "                                            transform=train_trafos)\n",
    "\n",
    "# we don't use data augmentations for the validation set\n",
    "val_dataset = utils.DatasetWithTransform(val_images, val_labels,\n",
    "                                          transform=utils.get_default_cifar_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOyeFQHZOz-x"
   },
   "outputs": [],
   "source": [
    "# sample augmentations\n",
    "def show_image(ax, image):\n",
    "    # need to go back to numpy array and WHC axis order\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    ax.imshow(image)\n",
    "\n",
    "n_samples = 8\n",
    "image_id = 0\n",
    "fig, ax = plt.subplots(1, n_samples, figsize=(18, 4))\n",
    "for sample in range(n_samples):\n",
    "    image, _ = train_dataset[0]\n",
    "    show_image(ax[sample], image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXIWFz0AzQcr"
   },
   "outputs": [],
   "source": [
    "# we reuse the model from the previous exercise\n",
    "# if you want you can also use a different CNN architecture that\n",
    "# you have designed in the tasks part of that exercise\n",
    "model = utils.SimpleCNN(10)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OejyDJzOzQc7"
   },
   "outputs": [],
   "source": [
    "# instantiate loaders and optimizer and start tensorboard\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=25)\n",
    "optimizer = Adam(model.parameters(), lr=1.e-3)\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ka_zkBLdzQdC"
   },
   "outputs": [],
   "source": [
    "# we have moved all the boilerplate for the full training procedure to utils now\n",
    "n_epochs = 10\n",
    "utils.run_cifar_training(model, optimizer,\n",
    "                         train_loader, val_loader,\n",
    "                         device=device, name='da1', \n",
    "                         n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CAINshVEzQdJ"
   },
   "outputs": [],
   "source": [
    "# evaluate the model on test data\n",
    "test_dataset = utils.make_cifar_test_dataset(cifar_dir)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25)\n",
    "predictions, labels = utils.validate(model, test_loader, nn.NLLLoss(),\n",
    "                                     device, step=0, tb_logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDsUV2PmzQdX"
   },
   "outputs": [],
   "source": [
    "print(\"Test accuracy:\")\n",
    "accuracy = metrics.accuracy_score(labels, predictions)\n",
    "print(accuracy)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "utils.make_confusion_matrix(labels, predictions, categories, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usicpPYzLCZg"
   },
   "source": [
    "## Normalization layers\n",
    "\n",
    "In addition to convolutional layers and pooling layers, another important part of neural networks are normalization layers.\n",
    "\n",
    "These layers keep their input normalized using a learned normalization. The first type of normalization introduced has been [BatchNorm](https://arxiv.org/abs/1502.03167), which we will now add to the CNN architecture from the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEehjuULdLt7"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNBatchNorm(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # the convolutions\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3)\n",
    "        # the pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # the normalization layers\n",
    "        self.bn1 = nn.BatchNorm2d(12)\n",
    "        self.bn2 = nn.BatchNorm2d(24)\n",
    "\n",
    "        # the fully connected part of the network\n",
    "        # after applying the convolutions and poolings, the tensor\n",
    "        # has the shape 24 x 6 x 6, see below\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(24 * 6 * 6, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 60),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(60, self.n_classes)\n",
    "        )\n",
    "        self.activation = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def apply_convs(self, x):\n",
    "      # input image has shape 3 x  32 x 32\n",
    "      x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "      # shape after conv: 12 x 28 x 28\n",
    "      # shape after pooling: 12 x 14 X 14\n",
    "      x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "      # shape after conv: 24 x 12 x 12\n",
    "      # shape after pooling: 24 x 6 x 6\n",
    "      return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.apply_convs(x)\n",
    "        x = x.view(-1, 24 * 6 * 6)\n",
    "        x = self.fc(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnlL64h1fBE0"
   },
   "outputs": [],
   "source": [
    "# instantiate model and optimizer\n",
    "model = CNNBatchNorm(10)\n",
    "model = model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8LI_7UjYfapv"
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "utils.run_cifar_training(model, optimizer,\n",
    "                         train_loader, val_loader,\n",
    "                         device=device, name='batch-norm', \n",
    "                         n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rkx-4mVRiG3l"
   },
   "outputs": [],
   "source": [
    "model = utils.load_checkpoin(\"best_checkpoint_batch-norm.tar\", model, optimizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfFBV5nYf1Fx"
   },
   "outputs": [],
   "source": [
    "predictions, labels = utils.validate(model, test_loader, nn.NLLLoss(),\n",
    "                                     device, step=0, tb_logger=None)\n",
    "\n",
    "print(\"Test accuracy:\")\n",
    "accuracy = metrics.accuracy_score(labels, predictions)\n",
    "print(accuracy)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "utils.make_confusion_matrix(labels, predictions, categories, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQUvUBL2zQdd"
   },
   "source": [
    "## Tasks and Questions\n",
    "\n",
    "Tasks:\n",
    "- Implement one or two additional augmentations and train the model again using these. You can use [the torchvision transformations](https://pytorch.org/docs/stable/torchvision/transforms.html) for inspiration.\n",
    "\n",
    "Questions:\n",
    "- Compare the model results in this exercise.\n",
    "- Can you think of any transformations that make use of symmetries/invariances not present here but present in other kinds of images (e.g. biomedical images)?\n",
    "\n",
    "Advanced:\n",
    "- Check out the other [normalization layers available in pytorch](https://pytorch.org/docs/stable/nn.html#normalization-layers). Which layers could be beneficial to BatchNorm here? Try training with them and see if this improves performance further."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "3_multi_layer_perceptron.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
