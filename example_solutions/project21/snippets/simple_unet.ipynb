{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/constantinpape/training-deep-learning-models-for-vison/blob/master/day3/unet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxJNtcLHyxK7"
   },
   "source": [
    "# Training a U-net\n",
    "\n",
    "The [U-net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) is a very popular model architecture for segmentation in bioimage analysis.\n",
    "Here, we will train a 2D U-net for nuclei segmentation, using data from the [Kaggle Nucleus Segmentation challenge](https://www.kaggle.com/c/data-science-bowl-2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5CNxOpgx-ey"
   },
   "source": [
    "## The libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IK0BTeevRv_N"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "import os\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from scipy.ndimage import binary_erosion\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFTBWHvqLYwL"
   },
   "source": [
    "## Data loading and preprocessing\n",
    "\n",
    "We first download amd explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k82ftNngK8Qf"
   },
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1EbvS10-83JGNE2nlBxIV42izY1TOr115\" -O kaggle_data.zip && rm -rf /tmp/cookies.txt\n",
    "!unzip -qq kaggle_data.zip && rm kaggle_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89JYxq-bL-Q3"
   },
   "source": [
    "Now make sure that the data was downloaded and extracted correctly. If everything went fine, you should have folders `nuclei_train_data` and `nuclei_val_data` in your working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dk-fAyl7L-Zj"
   },
   "outputs": [],
   "source": [
    "!ls -ltrh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tuRXDRJsMxVo"
   },
   "source": [
    "__TASK__: Use `ls` to explore the contents of both folders. Running `ls your_folder_name` should display you what is stored in the folder of your interest.\n",
    "\n",
    " How are the images stored? What format do they have? What about the ground truth (the annotation masks)? Which format are they stored in?\n",
    "\n",
    "Hint: you can use the following function to display the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehovgGj1NQrG"
   },
   "outputs": [],
   "source": [
    "def show_one_image(image_path):\n",
    "  image = imageio.imread(image_path)\n",
    "  plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ttIQl_WZd5Lo"
   },
   "source": [
    "The first step to training a network in pytorch is to write a dataset - a class that will fetch the training samples. You can find a dataset class for the nucleus data below. \n",
    "You can find more information about datasets in pytorch [here](https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset) and [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class).\n",
    "\n",
    "The main idea: any Dataset class should have two methods: `len` that returns the dataset length (the number of elements) and `getitem` that, given an index, returns input (image) and target (ground truth).\n",
    "\n",
    "Please have a look at the dataset implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6Zf2tpXh8Tk"
   },
   "outputs": [],
   "source": [
    "# any PyTorch dataset class should inherit the initial torch.utils.data.Dataset\n",
    "class NucleiDataset(Dataset):\n",
    "    \"\"\" A PyTorch dataset to load cell images and nuclei masks \"\"\"\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir  # the directory with all the training samples\n",
    "        self.samples = os.listdir(root_dir) # list the samples\n",
    "        self.transform = transform    # transformations to apply to both inputs and targets\n",
    "        #  transformations to apply just to inputs\n",
    "        self.inp_transforms = transforms.Compose([transforms.Grayscale(), # some of the images are RGB\n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize([0.5], [0.5])])\n",
    "        # transformations to apply just to targets\n",
    "        self.mask_transforms = transforms.ToTensor()\n",
    "\n",
    "    # get the total number of samples\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # fetch the training sample given its index\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.samples[idx],\n",
    "                                'images', self.samples[idx]+'.png')\n",
    "        # we'll be using Pillow library for reading files\n",
    "        # since many torchvision transforms operate on PIL images \n",
    "        image = Image.open(img_path)\n",
    "        image = self.inp_transforms(image)\n",
    "        masks_dir = os.path.join(self.root_dir, self.samples[idx], 'masks')\n",
    "        # masks directory has multiple images - one mask per nucleus\n",
    "        masks_list = os.listdir(masks_dir)\n",
    "        # create an empty array\n",
    "        mask = torch.zeros_like(image)\n",
    "        # iterate through the images to sum them up to one mask\n",
    "        for mask_name in masks_list:\n",
    "            one_nuclei_mask = Image.open(os.path.join(masks_dir, mask_name))\n",
    "            # erode the image by one pixel\n",
    "            # TASK: guess why is this done?\n",
    "            one_nuclei_mask = binary_erosion(one_nuclei_mask)\n",
    "            one_nuclei_mask = self.mask_transforms(one_nuclei_mask)\n",
    "            # add this nucleus to the mask\n",
    "            mask += one_nuclei_mask\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform([image, mask])\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wp6nJdOgvZBl"
   },
   "source": [
    "Now let's load the dataset and visualize it with a simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCjyh0Rbvf6Y"
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = 'nuclei_train_data'\n",
    "train_data = NucleiDataset(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fq0GitfQvnJF"
   },
   "outputs": [],
   "source": [
    "def show_random_dataset_image(dataset):\n",
    "    idx = np.random.randint(0, len(dataset))    # take a random sample\n",
    "    img, mask = dataset[idx]                    # get the image and the nuclei masks\n",
    "    f, axarr = plt.subplots(1, 2)               # make two plots on one figure\n",
    "    axarr[0].imshow(img[0])                     # show the image\n",
    "    axarr[1].imshow(mask[0])                    # show the masks\n",
    "    _ = [ax.axis('off') for ax in axarr]        # remove the axes\n",
    "    print('Image size is %s' % {img[0].shape})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYcY1Tg9vpLJ"
   },
   "outputs": [],
   "source": [
    "show_random_dataset_image(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JMHvV2rtvqyX"
   },
   "source": [
    "If you visualized many images, you will see that some of them are fairly large. What happens if we load them into memory and feed them to the mode? We might run out of memory. \n",
    "Consequently, when training networks on images or volumes one has to be careful about the input sizes. Also, for training in batches, i.e. feeding multiple images to the network at once, all images in the batch have to be of the same size. Hence all images need to be cropped or rescaled to the same size.\n",
    "\n",
    "For this exercise we implement a class that will apply a transformation `random crop`. Notice that we apply it to images and masks simultaneously to make sure they correspond despite the randomness.\n",
    "\n",
    "In case anybody is wondering why we have to bother to write a whole class for it instead of simply cropping the images directly in the dataset: we want to keep the code modular. We want to write one dataset object, and then we can try all the possible transforms with this one dataset. Similarly, we want to write one `RandomCrop` transform object, and then we can reuse it for any other image datasets we night have in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3X8AbTxGvyc1"
   },
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the input image and the output mask\"\"\"\n",
    "    def __init__(self, crop_size):\n",
    "        # check if the crop size is of a valid type\n",
    "        assert isinstance(crop_size, (int, tuple, list))\n",
    "        if isinstance(crop_size, int):\n",
    "            # if the crop size is an integer, we use the same for both dimensions\n",
    "            self.output_size = (crop_size, crop_size)\n",
    "        else:\n",
    "            assert len(crop_size) == 2\n",
    "            self.crop_size = crop_size\n",
    "\n",
    "    # this function makes our class callable \n",
    "    def __call__(self, sample):\n",
    "        # we need to crop both input and mask at the same time\n",
    "        assert len(sample) == 2\n",
    "        image, mask = sample\n",
    "        # the first dimension is channels, then width, then height\n",
    "        w, h = image.shape[1:]\n",
    "        new_w, new_h = self.output_size\n",
    "        # choose a random place to crop\n",
    "        top = np.random.randint(0, h - new_h) if h - new_h > 0 else 0\n",
    "        left = np.random.randint(0, w - new_w) if w - new_w > 0 else 0\n",
    "        # crop and return\n",
    "        image = image[:, left: left + new_w, top: top + new_h]\n",
    "        mask = mask[:, left: left + new_w, top: top + new_h]\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLA-L6sZw5TZ"
   },
   "source": [
    "PS: PyTorch already has quite a bunch of all possible data transforms, so if you need one, check [here](https://pytorch.org/docs/stable/torchvision/transforms.html). The biggest problem with them is that they are clearly separated into transforms applied to PIL images (remember, we initially load the images as PIL.Image?) and torch.tensors (remember, we converted the images into tensors by calling transforms.ToTensor()?). This can be quite annoying if for some reason you might need to transorm your images to tensors before applying any other transforms or you don't want to use PIL library at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UQ5hBaPNxJG1"
   },
   "outputs": [],
   "source": [
    "train_data = NucleiDataset(TRAIN_DATA_PATH, RandomCrop(256))\n",
    "train_loader = DataLoader(train_data, batch_size=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FX2SMLSCxNDy"
   },
   "outputs": [],
   "source": [
    "show_random_dataset_image(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sVH6172mxP0c"
   },
   "source": [
    "And the same for the validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qsrwcz9axSLG"
   },
   "outputs": [],
   "source": [
    "VAL_DATA_PATH = 'nuclei_val_data'\n",
    "val_data = NucleiDataset(VAL_DATA_PATH, RandomCrop(256))\n",
    "val_loader = DataLoader(val_data, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxltUmVLxazR"
   },
   "outputs": [],
   "source": [
    "show_random_dataset_image(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0pfxGQq2yhkD"
   },
   "source": [
    "## The model: U-net\n",
    "\n",
    "Now we need to define the architecture of the model to use. This time we will use a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) that has proven to steadily outperform the other architectures in segmenting biological and medical images.\n",
    "\n",
    "The image of the model precisely describes all the building blocks you need to use to create it. All of them can be found in the list of PyTorch layers (modules) [here](https://pytorch.org/docs/stable/nn.html#convolution-layers).\n",
    "\n",
    "The U-net has an encoder-decoder structure:\n",
    "In the encoder pass, the input image is successively downsampled via max-pooling. In the decoder pass it is upsampled again via transposed convolutions.\n",
    "In adddition, it has skip connections, that bridge the output from an encoder to the corresponding decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lf7Y_PmU7sQ3"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\" UNet implementation\n",
    "    Arguments:\n",
    "      in_channels: number of input channels\n",
    "      out_channels: number of output channels\n",
    "      final_activation: activation applied to the network output\n",
    "    \"\"\"\n",
    "    \n",
    "    # _conv_block and _upsampler are just helper functions to\n",
    "    # construct the model.\n",
    "    # encapsulating them like so also makes it easy to re-use\n",
    "    # the model implementation with different architecture elements\n",
    "    \n",
    "    # Convolutional block for single layer of the decoder / encoder\n",
    "    # we apply to 2d convolutions with relu activation\n",
    "    def _conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                             nn.ReLU(),\n",
    "                             nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                             nn.ReLU())       \n",
    "\n",
    "\n",
    "    # upsampling via transposed 2d convolutions\n",
    "    def _upsampler(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels,\n",
    "                                kernel_size=2, stride=2)\n",
    "    \n",
    "    def __init__(self, in_channels=1, out_channels=1, \n",
    "                 final_activation=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # the depth (= number of encoder / decoder levels) is\n",
    "        # hard-coded to 4\n",
    "        self.depth = 4\n",
    "\n",
    "        # the final activation must either be None or a Module\n",
    "        if final_activation is not None:\n",
    "            assert isinstance(final_activation, nn.Module), \"Activation must be torch module\"\n",
    "        \n",
    "        # all lists of conv layers (or other nn.Modules with parameters) must be wraped\n",
    "        # itnto a nn.ModuleList\n",
    "        \n",
    "        # modules of the encoder path\n",
    "        self.encoder = nn.ModuleList([self._conv_block(in_channels, 16),\n",
    "                                      self._conv_block(16, 32),\n",
    "                                      self._conv_block(32, 64),\n",
    "                                      self._conv_block(64, 128)])\n",
    "        # the base convolution block\n",
    "        self.base = self._conv_block(128, 256)\n",
    "        # modules of the decoder path\n",
    "        self.decoder = nn.ModuleList([self._conv_block(256, 128),\n",
    "                                      self._conv_block(128, 64),\n",
    "                                      self._conv_block(64, 32),\n",
    "                                      self._conv_block(32, 16)])\n",
    "        \n",
    "        # the pooling layers; we use 2x2 MaxPooling\n",
    "        self.poolers = nn.ModuleList([nn.MaxPool2d(2) for _ in range(self.depth)])\n",
    "        # the upsampling layers\n",
    "        self.upsamplers = nn.ModuleList([self._upsampler(256, 128),\n",
    "                                         self._upsampler(128, 64),\n",
    "                                         self._upsampler(64, 32),\n",
    "                                         self._upsampler(32, 16)])\n",
    "        # output conv and activation\n",
    "        # the output conv is not followed by a non-linearity, because we apply\n",
    "        # activation afterwards\n",
    "        self.out_conv = nn.Conv2d(16, out_channels, 1)\n",
    "        self.activation = final_activation\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        # apply encoder path\n",
    "        encoder_out = []\n",
    "        for level in range(self.depth):\n",
    "            x = self.encoder[level](x)\n",
    "            encoder_out.append(x)\n",
    "            x = self.poolers[level](x)\n",
    "\n",
    "        # apply base\n",
    "        x = self.base(x)\n",
    "        \n",
    "        # apply decoder path\n",
    "        encoder_out = encoder_out[::-1]\n",
    "        for level in range(self.depth):\n",
    "            x = self.upsamplers[level](x)\n",
    "            x = self.decoder[level](torch.cat((x, encoder_out[level]), dim=1))\n",
    "        \n",
    "        # apply output conv and activation (if given)\n",
    "        x = self.out_conv(x)\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_MTjpyXIv8bm"
   },
   "source": [
    "## Loss and distance metrics\n",
    "\n",
    "Next, we need to define the loss function - a metric that will tell us how close we are to the desired output. This metric should be differentiable, since it needs to be backpropagated. The are [multiple losses](https://lars76.github.io/2018/09/27/loss-functions-for-segmentation.html) we could use for the segmentation task.\n",
    "\n",
    "Take a moment to think which one is better to use for our exercise. If you are not sure, don't forget that you can always google! Before you start implementing the loss yourself, take a look at the [losses](https://pytorch.org/docs/stable/nn.html#loss-functions) already implemented in PyTorch. You can also look for implementations on GitHub.\n",
    "\n",
    "__TASK__: implement your loss (or take one from pytorch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WaSLoe-IvhI8"
   },
   "outputs": [],
   "source": [
    "YOUR_LOSS_NAME = # implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FRujJEY1xg2q"
   },
   "source": [
    "We will use the [Dice Coefficeint](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) to evaluate the network predictions.\n",
    "We can use it for validation if we interpret set $a$ as predictions and $b$ as labels. It is often used to evaluate segmentations with sparse foreground, because the denominator normalizes by the number of foreground pixels.\n",
    "The Dice Coefficient is closely related to Jaccard Index / Intersection over Union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k4BbxNxVxVuE"
   },
   "outputs": [],
   "source": [
    "# sorensen dice coefficient implemented in torch\n",
    "# the coefficient takes values in [0, 1], where 0 is\n",
    "# the worst score, 1 is the best score\n",
    "class DiceCoefficient(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    # the dice coefficient of two sets represented as vectors a, b ca be \n",
    "    # computed as (2 *|a b| / (a^2 + b^2))\n",
    "    def forward(self, prediction, target):\n",
    "        intersection = (prediction * target).sum()\n",
    "        denominator = (prediction * prediction).sum() + (target * target).sum()\n",
    "        return (2 * intersection / denominator.clamp(min=self.eps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bfq6uCE3yfjo"
   },
   "source": [
    "## Training\n",
    "\n",
    "Let's start with writing training and validation functions. __TASK__: fix in all the TODOs to make the function run. You can use the function from the classification exercises as a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vHmBksOzpl9"
   },
   "outputs": [],
   "source": [
    "# apply training for one epoch\n",
    "def train(model, loader, optimizer, loss_function,\n",
    "          epoch, log_interval=100, log_image_interval=20, tb_logger=None):\n",
    "\n",
    "    # set the model to train mode\n",
    "    TODO: YOUR CODE HERE\n",
    "    # iterate over the batches of this epoch\n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "        # move input and target to the active device (either cpu or gpu)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # zero the gradients for this iteration\n",
    "        TODO: YOUR CODE HERE\n",
    "        \n",
    "        # apply model, calculate loss and run backwards pass\n",
    "        TODO: YOUR CODE HERE\n",
    "        \n",
    "        # log to console\n",
    "        if batch_id % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                  epoch, batch_id * len(x),\n",
    "                  len(loader.dataset),\n",
    "                  100. * batch_id / len(loader), loss.item()))\n",
    "\n",
    "       # log to tensorboard\n",
    "        if tb_logger is not None:\n",
    "            step = epoch * len(loader) + batch_id\n",
    "            tb_logger.add_scalar(tag='train_loss', scalar_value=loss.item(), global_step=step)\n",
    "            # check if we log images in this iteration\n",
    "            if step % log_image_interval == 0:\n",
    "                tb_logger.add_images(tag='input', img_tensor=x.to('cpu'), global_step=step)\n",
    "                tb_logger.add_images(tag='target', img_tensor=y.to('cpu'), global_step=step)\n",
    "                tb_logger.add_images(tag='prediction', img_tensor=prediction.to('cpu').detach(), global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdCGd3-N3gpI"
   },
   "outputs": [],
   "source": [
    "# run validation after training epoch\n",
    "def validate(model, loader, loss_function, metric, step=None, tb_logger=None):\n",
    "    # set model to eval mode\n",
    "    TODO: YOUR CODE HERE\n",
    "    # running loss and metric values\n",
    "    val_loss = 0\n",
    "    val_metric = 0\n",
    "    \n",
    "    # disable gradients during validation\n",
    "    TODO: YOUR CODE HERE\n",
    "        \n",
    "        # iterate over validation loader and update loss and metric values\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            TODO: YOUR CODE HERE\n",
    "    \n",
    "    # normalize loss and metric\n",
    "    val_loss /= len(loader)\n",
    "    val_metric /= len(loader)\n",
    "    \n",
    "    if tb_logger is not None:\n",
    "        assert step is not None, \"Need to know the current step to log validation results\"\n",
    "        tb_logger.add_scalar(tag='val_loss', scalar_value=val_loss, global_step=step)\n",
    "        tb_logger.add_scalar(tag='val_metric', scalar_value=val_metric, global_step=step)\n",
    "        # we always log the last validation images\n",
    "        tb_logger.add_images(tag='val_input', img_tensor=x.to('cpu'), global_step=step)\n",
    "        tb_logger.add_images(tag='val_target', img_tensor=y.to('cpu'), global_step=step)\n",
    "        tb_logger.add_images(tag='val_prediction', img_tensor=prediction.to('cpu'), global_step=step)\n",
    "        \n",
    "    print('\\nValidate: Average loss: {:.4f}, Average Metric: {:.4f}\\n'.format(val_loss, val_metric))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MMARc1MBzsCT"
   },
   "source": [
    "\n",
    "This time we will use GPU to train faster. Please make sure that your Notebook is running on GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGJeKzlmvn-A"
   },
   "outputs": [],
   "source": [
    "# check if we have  a gpu\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Tai5zj8U6kg"
   },
   "outputs": [],
   "source": [
    "# start a tensorboard writer\n",
    "logger = SummaryWriter('runs/Unet')\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RUwWEAFVHLz"
   },
   "outputs": [],
   "source": [
    "# build a default unet with sigmoid activation\n",
    "# to normalize predictions to [0, 1]\n",
    "net = UNet(1, 1, final_activation=nn.Sigmoid())\n",
    "# move the model to GPU\n",
    "net = net.to(device)\n",
    "\n",
    "# use adam optimizer\n",
    "TODO: YOUR CODE HERE\n",
    "\n",
    "# build the dice coefficient metric\n",
    "metric = DiceCoefficient()\n",
    "\n",
    "# train for 25 epochs\n",
    "# during the training you can inspect the \n",
    "# predictions in the tensorboard\n",
    "n_epochs = 25\n",
    "for epoch in range(n_epochs):\n",
    "    # train\n",
    "    TODO: YOUR CODE HERE\n",
    "    step = epoch * len(train_loader.dataset)\n",
    "    # validate\n",
    "    TODO: YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jq9kGHGgpeNT"
   },
   "source": [
    "## Additional Exercises \n",
    "\n",
    "1. Implement and compare at least 2 of the following architecture variants of the U-Net:\n",
    "    * use [Dropout](https://pytorch.org/docs/stable/nn.html#dropout-layers) in the decoder path\n",
    "    * use [BatchNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm2d) to normalize layer inputs\n",
    "    * use [GroupNorm](https://pytorch.org/docs/stable/nn.html#torch.nn.GroupNorm) to normalize convolutional group inputs\n",
    "    * use [ELU-Activations](https://pytorch.org/docs/stable/nn.html#torch.nn.ELU) instead of ReLU-Activations\n",
    "\n",
    "2. Use the Dice coefficient as loss function. Before we only used it for validation, but it is differentiable and can thus also be used as loss. Compare to the results from exercise 2. \n",
    "Hint: The optimizer we use finds minima of the loss, but the minimal value for the Dice coefficient corresponds to a bad segmentation. How do we need to change the Dice coefficient to use it as loss nonetheless?\n",
    "\n",
    "3. Add one more layer to the U-net model (currently it has 4). Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cEyrT9Ohqp-C"
   },
   "source": [
    "## Advanced Exercises\n",
    "\n",
    "1. Visualize the graph (model) that we are using with TensorBoard\n",
    "2. Write your own data transform (e.g., RandomRotate)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Untitled",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
