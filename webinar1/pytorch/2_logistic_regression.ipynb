{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4-FojgQH72pf"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/constantinpape/dl-teaching-resources/blob/main/exercises/classification/2_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f38lqMlA72pi"
   },
   "source": [
    "# Logistic Regression on CIFAR10\n",
    "\n",
    "Now that we have seen how to load image data for classification and wrap it in a torch dataset, we will train simple classification models with pytorch.\n",
    "In this notebook, we will start with [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "that learns to classifiy the images based on the pixel values.\n",
    "Note that logistic regression corresponds to an artificial neural network with a single, fully connected, layer.\n",
    "We will also train a model with \"hard-coded\" convolutional features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1pXIj82rPDK"
   },
   "source": [
    "## Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BTaMFNwx72pl"
   },
   "outputs": [],
   "source": [
    "# load tensorboard extension\n",
    "# we will need this later in the notebook to monitor the training progress\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CRbxZYJ572pz"
   },
   "outputs": [],
   "source": [
    "# import torch and other libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cFxr0guN72p8"
   },
   "outputs": [],
   "source": [
    "# check if we have gpu support\n",
    "# if you run these exercises on colab, you can use the free colab gpus:\n",
    "# (however they are not activated by default)\n",
    "# to activate the gpu on colab, go to 'Runtime->Change runtime type'. \n",
    "# Then select 'GPU' in 'Hardware accelerator' and click 'Save'\n",
    "have_gpu = torch.cuda.is_available()\n",
    "# we need to define the device for torch, yadda yadda\n",
    "if have_gpu:\n",
    "    print(\"GPU is available\")\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    print(\"GPU is not available, training will run on the CPU\")\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-xJoQJ272qH"
   },
   "outputs": [],
   "source": [
    "# we refactor some codes from previous notebooks into a python file 'utils.py'\n",
    "# to reuse it in later notebooks.\n",
    "# run this in google colab to get the utils.py file\n",
    "!wget https://raw.githubusercontent.com/constantinpape/dl-teaching-resources/main/exercises/classification/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TIfA0SJP72qR"
   },
   "outputs": [],
   "source": [
    "# import the utils file\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VdtqT7aV72qZ"
   },
   "outputs": [],
   "source": [
    "cifar_dir = './cifar10'\n",
    "!cifar2png cifar10 cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tWWWX_cg26ed"
   },
   "outputs": [],
   "source": [
    "categories = os.listdir('./cifar10/train')\n",
    "categories.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zgfos1rX72qj"
   },
   "source": [
    "## Prepare the data\n",
    "\n",
    "We have already seen how to create a torch dataset for the cifar dataset in the previous notebook. We will repeat this here, but now split our data into training and validation data beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EaBoP3yB72qk"
   },
   "outputs": [],
   "source": [
    "# load images and labels\n",
    "images, labels = utils.load_cifar(os.path.join(cifar_dir, 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eY8zcSfW72qt"
   },
   "outputs": [],
   "source": [
    "# split the data into train and validation\n",
    "# using 'train_test_split' from sklearn the data is shuffled and\n",
    "# stratified, i.e. the same number of samples per classes \n",
    "# is present in the train and validation split\n",
    "n_images = len(images)\n",
    "(train_images, val_images,\n",
    " train_labels, val_labels) = train_test_split(images, labels, shuffle=True,\n",
    "                                              test_size=0.15, stratify=labels)\n",
    "assert len(train_images) == len(train_labels)\n",
    "assert len(val_images) == len(val_labels)\n",
    "assert len(train_images) + len(val_images) == n_images\n",
    "\n",
    "print(\"Split cifar into training and validation data:\")\n",
    "print(\"Have\", len(train_images), \"training images and\", len(val_images), \"validation images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f2kcAP8D72qy"
   },
   "outputs": [],
   "source": [
    "# create the torch datasets for training and validation\n",
    "from functools import partial\n",
    "\n",
    "trafos = [utils.to_channel_first, utils.normalize, utils.to_tensor]\n",
    "trafos = partial(utils.compose, transforms=trafos)\n",
    "\n",
    "train_dataset = utils.DatasetWithTransform(train_images, train_labels,\n",
    "                                           transform=trafos)\n",
    "\n",
    "val_dataset = utils.DatasetWithTransform(val_images, val_labels,\n",
    "                                         transform=trafos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLPgiI5j72q4"
   },
   "source": [
    "## Training and validation function\n",
    "\n",
    "Now, we will write functions to train a model for one epoch and a function to validate it after an epoch.\n",
    "Here, an `epoch` means iterating through the available training data once.\n",
    "\n",
    "In order to monitor the progress during training, we will use [tensorboard](https://www.tensorflow.org/tensorboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zvhCbhc72q5"
   },
   "outputs": [],
   "source": [
    "def train(model, loader, \n",
    "          loss_function, optimizer,\n",
    "          device, epoch,\n",
    "          tb_logger, log_image_interval=100):\n",
    "    \"\"\" Train model for one epoch.\n",
    "    \n",
    "    Parameters:\n",
    "    model - the model we are training\n",
    "    loader - the data loader that provides the training data\n",
    "        (= pairs of images and labels)\n",
    "    loss_function - the loss function that will be optimized\n",
    "    optimizer - the optimizer that is used to update the network parameters\n",
    "        by backpropagation of the loss\n",
    "    device - the device used for training. this can either be the cpu or gpu\n",
    "    epoch - which trainin eppch are we in? we keep track of this for logging\n",
    "    tb_logger - the tensorboard logger, it is used to communicate with tensorboard\n",
    "    log_image_interval - how often do we send images to tensborboard?\n",
    "    \"\"\"\n",
    "\n",
    "    # set model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # iterate over the training batches provided by the loader\n",
    "    n_batches = len(loader)\n",
    "    for batch_id, (x, y) in enumerate(loader):\n",
    "       \n",
    "        # send data and target tensors to the active device\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # set the gradients to zero, to start with \"clean\" gradients\n",
    "        # in this training iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # apply the model to get the prediction\n",
    "        prediction = model(x)\n",
    "        \n",
    "        # calculate the loss (negative log likelihood loss)\n",
    "        # the loss function expects a 1d tensor, but we get a 2d tensor\n",
    "        # with singleton second dimension, so we get rid of this dimension\n",
    "        loss_value = loss_function(prediction, y[:, 0])\n",
    "        \n",
    "        # calculate the gradients (`loss.backward()`) \n",
    "        # and apply them to the model parameters with\n",
    "        # to our optimizer (`optimizer.step()`)\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # log the loss value to tensorboard\n",
    "        step = epoch * n_batches + batch_id\n",
    "        tb_logger.add_scalar(tag='train-loss', \n",
    "                             scalar_value=loss_value.item(),\n",
    "                             global_step=step)\n",
    "        \n",
    "        # check if we log images, and if we do then send the\n",
    "        # current image to tensorboard\n",
    "        if log_image_interval is not None and step % log_image_interval == 0:\n",
    "            tb_logger.add_images(tag='input', \n",
    "                                 img_tensor=x.to('cpu'),\n",
    "                                 global_step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyBCeRgm72rA"
   },
   "outputs": [],
   "source": [
    "# the validation function takes the model, runs prediction for\n",
    "# all images provided by the loader and evaluates the results.\n",
    "def validate(model, loader, loss_function, \n",
    "             device, step, tb_logger=None):\n",
    "    \"\"\"\n",
    "    Validate the model predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    model - the model to be evaluated\n",
    "    loader - the loader providing images and labels\n",
    "    loss_function - the loss function\n",
    "    device - the device used for prediction (cpu or gpu)\n",
    "    step - the current training step. we need to know this for logging\n",
    "    tb_logger - the tensorboard logger. if 'None', logging is disabled\n",
    "    \"\"\"\n",
    "    # set the model to eval mode\n",
    "    model.eval()\n",
    "    n_batches = len(loader)\n",
    "   \n",
    "    # we record the loss and the predictions / labels for all samples\n",
    "    mean_loss = 0\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    # the model parameters should not be updated during validation\n",
    "    # torch.no_grad disables gradient updates in its scope\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            prediction = model(x)\n",
    "            \n",
    "            # update the loss \n",
    "            mean_loss += loss_function(prediction, y[:, 0]).item()\n",
    "    \n",
    "            # compute the most likely class predictions\n",
    "            # note that 'max' returns a tuple with the \n",
    "            # index of the maximun value (which correponds to the predicted class)\n",
    "            # as second entry\n",
    "            prediction = prediction.max(1, keepdim=True)[1]\n",
    "\n",
    "            # store the predictions and labels\n",
    "            predictions.append(prediction[:, 0].to('cpu').numpy())\n",
    "            labels.append(y[:, 0].to('cpu').numpy())\n",
    "      \n",
    "    # predictions and labels to numpy arrays\n",
    "    predictions = np.concatenate(predictions)\n",
    "    labels = np.concatenate(labels)\n",
    "    \n",
    "    # log the validation results if we have a tensorboard\n",
    "    if tb_logger is not None:\n",
    "        \n",
    "        accuracy_error = 1. - metrics.accuracy_score(labels, predictions)\n",
    "        mean_loss /= n_batches\n",
    "        \n",
    "        tb_logger.add_scalar(tag=\"validation-error\",\n",
    "                             global_step=step,\n",
    "                             scalar_value=accuracy_error)\n",
    "        tb_logger.add_scalar(tag=\"validation-loss\",\n",
    "                             global_step=step,\n",
    "                             scalar_value=mean_loss)\n",
    "\n",
    "    # return all predictions and labels for further evaluation\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sAfcGqCH72rG"
   },
   "source": [
    "## Model\n",
    "\n",
    "In pytorch models are defined by a class that inherits from `nn.Module`.\n",
    "This allows to add member variables (that are sub-classes of `nn.Module` themselves) so that these are handled correctly in auto differentiation.\n",
    "The forward pass of a model is defined in the member function `forward`, the backward pass will be automatically generated.\n",
    "\n",
    "Here, we define a simple network consisting of a single fully connected layer that receives the image as input.\n",
    "When trained with cross entropy loss, this model corresponds to a logistic regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KTsMI30_72rH"
   },
   "outputs": [],
   "source": [
    "# define logistic regression model\n",
    "class LogisticRegressor(nn.Module):\n",
    "    def __init__(self, n_pixels, n_classes):\n",
    "        # the parent class 'nn.Module' needs to be initialised so\n",
    "        # that all members that are subclasses of nn.Module themselves\n",
    "        #  are correctly handled in autodiff\n",
    "        super().__init__()\n",
    "        self.n_pixels = n_pixels\n",
    "        self.n_classes = n_classes\n",
    "        # nn.Sequential applies its arguments one after the other \n",
    "        self.log_reg = nn.Sequential(\n",
    "            # nn.Linear instantiates a fully connected layer, the first argument\n",
    "            # specifies the number of input units, the second argument the number\n",
    "            # of output units\n",
    "            nn.Linear(self.n_pixels, self.n_classes),\n",
    "            # logarithmic softmax activation.\n",
    "            # the combination of LogSoftmax and negative log-likelihood loss\n",
    "            # (see below) corresponds to training with Cross Entropy, but is\n",
    "            # numerically more stable\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # reshape the input to be 1d instead of 2d,\n",
    "        # which is required for fully connected layers\n",
    "        x = x.view(-1, self.n_pixels)\n",
    "        x = self.log_reg(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVKvJhyg72rO"
   },
   "source": [
    "## Training\n",
    "\n",
    "Now we will put everything together and instantiate the model, loss function and run training and validation for a couple of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "caOIkSPO72rP"
   },
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "n_pixels = images[0].size  # number of pixels = 3 * 32 * 32\n",
    "n_classes = 10    \n",
    "model = LogisticRegressor(n_pixels, n_classes)\n",
    "\n",
    "# with '.to' the parameters of the model (or any other subclass of torch.nn.Module)\n",
    "# are sent to the specified device, which can be the cpu or a gpu\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEkTsnJ872rV"
   },
   "outputs": [],
   "source": [
    "# instantiate the DataLoaders\n",
    "# torch DataLoaders provide data from a dataset so that it can be ingested by the model.\n",
    "# the most important aspect is that the data is stacked along the\n",
    "# batch axis (which is the first axis in torch convention)\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# the loader for training: we train with 4 batches per sample\n",
    "# and shuffle the data\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "# the loader for validation: we use a larger batch size and \n",
    "# don't need to shuffle the data\n",
    "val_loader = DataLoader(val_dataset, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBxGon7_72ra"
   },
   "outputs": [],
   "source": [
    "# instantiate the optimizer\n",
    "# the optimizer applies the gradients of the loss to the model parameters by some\n",
    "# variant of stochastic gradient descent\n",
    "# here, we use the Adam optimizer \n",
    "# https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "from torch.optim import Adam\n",
    "\n",
    "# the most important parameter to the optimizer is the \n",
    "# learning rate, which is multiplied as a factor to the gradients during\n",
    "# the parameter update\n",
    "optimizer = Adam(model.parameters(), lr=1.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tokPOzj72rp"
   },
   "outputs": [],
   "source": [
    "# instantiate the loss function \n",
    "# here, we use the negative log likelihood loss\n",
    "# combined with the LogSoftmax activation (see model definition)\n",
    "# this corresponds to the CrossEntropy loss, but is numerically more stable\n",
    "loss_function = nn.NLLLoss()\n",
    "loss_function.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "elDaXpDa72ru"
   },
   "outputs": [],
   "source": [
    "# clear the tensorboard logs if they exist\n",
    "# skip this step if you want to keep the logs of a previous\n",
    "from shutil import rmtree\n",
    "try:\n",
    "    rmtree('runs/log_reg')\n",
    "except OSError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyeFHayN72ry"
   },
   "outputs": [],
   "source": [
    "# instantiate the tensorboard logger that will be passed to the\n",
    "# training and validation functions\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tb_logger = SummaryWriter('runs/log_reg')\n",
    "\n",
    "# this magic command will open tensorboard inside of our notebook\n",
    "# click on the refresh symbol after the training (see below) has finished\n",
    "# to see the training progress log\n",
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zZkWkQA72r2"
   },
   "outputs": [],
   "source": [
    "# run training for a couple of epochs\n",
    "from tqdm import trange  # progress bar for the overall training\n",
    "n_epochs = 6\n",
    "for epoch in trange(n_epochs):\n",
    "    train(model, train_loader, loss_function, optimizer,\n",
    "          device, epoch, tb_logger=tb_logger)\n",
    "    step = (epoch + 1) * len(train_loader)\n",
    "    validate(model, val_loader, loss_function,\n",
    "             device, step,\n",
    "             tb_logger=tb_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8NRf3qT72r6"
   },
   "source": [
    "## Check the model on test data\n",
    "\n",
    "Now that we have a trained model, we need to evaluate it on unseen data, i.e. data that was neither used for training the model nor validating it during training. For this, we will use the separate cifar10 test dataset.\n",
    "\n",
    "To validate the model predictions we will compute the model accuracy and plot the confusion matrix, to get a sense of the types of errors made by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fksUdVgH72r8"
   },
   "outputs": [],
   "source": [
    "test_images, test_labels = utils.load_cifar(os.path.join(cifar_dir, 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6aqGOq3f72sB"
   },
   "outputs": [],
   "source": [
    "# we build a test dataset and loader and reuse the validation\n",
    "# function to get all predictions and labels for our test-set\n",
    "test_dataset = utils.DatasetWithTransform(test_images, test_labels,\n",
    "                                          transform=trafos)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25)\n",
    "\n",
    "test_predictions, test_labels = validate(model, test_loader, loss_function,\n",
    "                                         device, 0, tb_logger=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyJXzg7472sH"
   },
   "outputs": [],
   "source": [
    "# to evaluate the model we compute the overall accuracy and \n",
    "# the class confusion matrix\n",
    "\n",
    "accuracy = metrics.accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test accuracy\")\n",
    "print(accuracy)\n",
    "print()\n",
    "\n",
    "# we have implemented generating the confusion matrix in the utils already\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "utils.make_confusion_matrix(test_labels,\n",
    "                            test_predictions,\n",
    "                            categories, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pLfN0fWL72sM"
   },
   "source": [
    "## Logistic Regression with preset filters\n",
    "\n",
    "Now, we will try to improve the model performance by presenting additional features to the model. For this, we can compute convolutional filters (see previous notebook) on the image and present the filter responses as (additional) features to the model.\n",
    "\n",
    "Hopefully, these features provide additional context to the model that can improve its performance.\n",
    "\n",
    "We will implement these preset filters using the transform mechanism we have implemented in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHCR0lCe72sN"
   },
   "outputs": [],
   "source": [
    "# apply a list of filters as on the fly transformation. \n",
    "def apply_filters(image, target, filter_list, keep_image=False):    \n",
    "    filtered = [image] if keep_image else [] \n",
    "    for filter_function in filter_list:\n",
    "        filtered.append(filter_function(image))\n",
    "    data = np.concatenate(filtered, axis=-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dRVVfGgW72sS"
   },
   "outputs": [],
   "source": [
    "# build dataset with filter transformations\n",
    "\n",
    "import skimage.filters as filters\n",
    "\n",
    "filters = [filters.gaussian, filters.laplace]\n",
    "filters = partial(apply_filters,\n",
    "                  filter_list=filters,\n",
    "                  keep_image=True)\n",
    "\n",
    "trafos = [\n",
    "    filters,\n",
    "    utils.normalize,\n",
    "    utils.to_channel_first,\n",
    "    utils.to_tensor\n",
    "]\n",
    "trafos = partial(utils.compose, transforms=trafos)\n",
    "\n",
    "train_dataset = utils.DatasetWithTransform(train_images, train_labels,\n",
    "                                           transform=trafos)\n",
    "\n",
    "val_dataset = utils.DatasetWithTransform(val_images, val_labels,\n",
    "                                         transform=trafos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NR7bA6M72sV"
   },
   "outputs": [],
   "source": [
    "# make loaders for training and validation data\n",
    "batch_size = 4\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTgOFwPJ72sa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run training for the model with preset filters\n",
    "\n",
    "n_pixels = 9 * 32 * 32  # number channels * number pixels\n",
    "n_classes = 10    \n",
    "model = LogisticRegressor(n_pixels, n_classes)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1.e-3)\n",
    "\n",
    "# you can find the results of this training run in the tensorboard\n",
    "# above as well, they will have the name 'log_reg_filters1' and\n",
    "# will be differently colored compared to the first model\n",
    "tb_logger = SummaryWriter('runs/log_reg_filters1')\n",
    "\n",
    "n_epochs = 6\n",
    "for epoch in trange(n_epochs):\n",
    "    train(model, train_loader, loss_function, optimizer,\n",
    "          device, epoch, tb_logger=tb_logger, log_image_interval=None)\n",
    "    step = (epoch + 1) * len(train_loader)\n",
    "    validate(model, val_loader, loss_function, device, step,\n",
    "             tb_logger=tb_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sZgHXw8NAO9o"
   },
   "outputs": [],
   "source": [
    "# evaluate the new model\n",
    "test_dataset = utils.DatasetWithTransform(test_images, test_labels,\n",
    "                                          transform=trafos)\n",
    "test_loader = DataLoader(test_dataset, batch_size=25)\n",
    "\n",
    "test_predictions, test_labels = validate(model, test_loader, loss_function,\n",
    "                                         device, 0, tb_logger=None)\n",
    "accuracy = metrics.accuracy_score(test_labels, test_predictions)\n",
    "print(\"Test accuracy\")\n",
    "print(accuracy)\n",
    "print()\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(8, 8))\n",
    "utils.make_confusion_matrix(test_labels,\n",
    "                            test_predictions,\n",
    "                            categories, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZLBaSWWK72sc"
   },
   "source": [
    "## Tasks and Questions:\n",
    "\n",
    "Tasks:\n",
    "- The `learning_rate` (the parameter `lr` in Adam(model.parameters(), lr=1.e-3)) is a very important hyperparameter for training neural networks. Train the logistic regression model with filters using a few different learning rates and see how this changes the training curves and model performance. \n",
    "\n",
    "Questions:\n",
    "- What accuracy do the different models reach?\n",
    "- Which accuracy do you expect by guessing?\n",
    "- Can you find any systematic errors from the confusion matrix?\n",
    "\n",
    "Advanced:\n",
    "- Train a model with more preset filters and compare the different models via tensorboard and on the test dataset.\n",
    "- The filters we have used here can be expressed as convolutions.\n",
    "    - Express the `gaussian` filter using [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) and train a model using these filters.\n",
    "    - Can you also explace the `laplace` filter?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "2_logistic_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3.bkp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
