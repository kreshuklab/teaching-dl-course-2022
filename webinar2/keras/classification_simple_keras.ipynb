{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature classification in tensorflow/keras with a simple MLP  \n",
    "\n",
    "This notebook demonstrates how to perform a basic feature classification example with a Multi layer perceptron (MLP) using tensorflow/keras.\n",
    "\n",
    "Although this classification task is very simple, it will serve as a basic demonstration of a typical workflow in tensorflow/keras. \n",
    "\n",
    "\n",
    "![](_images/class_bound.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "cmap = 'coolwarm'\n",
    "colors = np.array(tuple(plt.cm.get_cmap(cmap)(float(v)) for v in (0,1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "Lets first define the Dataset for which we use some demo data from `scikit-learn` that provides some datasets of  points having 2 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "XA, YA = datasets.make_moons(n_samples=200, noise=0.1)\n",
    "XB, YB = datasets.make_circles(n_samples=200,noise=.1, factor=.4)\n",
    "\n",
    "off = 1.3\n",
    "X = np.concatenate([XA+off*np.array([-2,1]),XB+off*np.array([1,2]), XA+off*np.array([1,-1]),XB+off*np.array([-1,-2])], axis=0)\n",
    "Y = np.concatenate([YA,YB, YA, YB], axis=0)\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in (0,1): \n",
    "    plt.scatter(*X[Y==i].T, color = colors[i], marker='o', alpha=.5, label = f'class {i+1}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the shape of the input data `X` is `(number_of_samples, 2)` (with the feature dimension `2`) and that of the target `Y` is simple `(number_of_samples, 1)` containing the class number as an integer. \n",
    "\n",
    "In general for tensorflow, the input/output data has to have the shape `(N, ... , C)` with the first dimension being the batch/sample dimension `N` and the last dimension being the channel dimension `C`. If you later segment images or volumes, they would have to have dimensions `(N, Height, Width, C)` (2D images) or   `(N, Depth, Height, Width, C)` (volumes), with the channel dimension always being last. Note that this is different in `pytorch`, where the channel dimension is the second dimension.\n",
    "\n",
    "| Data | `tensorflow` | `pytorch` |\n",
    "| --- | --- | --- |\n",
    "| pure features | `(N,C)` | `(N,C)` |\n",
    "| images (2D) | `(N,H,W,C)` | `(N,C,H,W)` |\n",
    "| volumes (3D) | `(N,D,H,W,C)` | `(N,C,D,H,W)` |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X.shape=}, {Y.shape=}\")\n",
    "print(f\"first input element: {X[0]=} with class label {Y[0]=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets split our data into training data (which will be used for optimizing the model parameters) and validation data (which will be used to monitor the model performance on unseen data). We will use a 80/20% split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xt, Xv, Yt, Yv =  train_test_split(X,Y,train_size = 0.8)\n",
    "\n",
    "print(f\"{Xt.shape=}, {Yt.shape=}, {Xv.shape=}, {Yv.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "for i in (0,1): \n",
    "    plt.scatter(*Xt[Yt==i].T, color = colors[i], marker='o', alpha=.5, label = f'Train: class {i+1}')\n",
    "    plt.scatter(*Xv[Yv==i].T, color = colors[i], marker='^', label = f'Valid: class {i+1}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model \n",
    "\n",
    "\n",
    "We will use a simple multi layer perceptron (MLP), which consists of multiple dense (fully connected) hidden layers with `relu` activations and a final output layer that outputs the class probability with a `softmax` activation function (as we only have 2 classes, this is equivalent to a single output and `sigmoid` final activation).   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(activation = \"relu\", n_neurons=64, n_hidden = 3):\n",
    "    np.random.seed(42)\n",
    "    inp = Input((2,), name = \"input\")\n",
    "    layer = inp \n",
    "    for n in range(n_hidden):\n",
    "        layer = Dense(n_neurons,activation=activation, name= \"hidden_%s\"%(n+1))(layer)\n",
    "        \n",
    "    out = Dense(2, activation='softmax', name =  \"output\")(layer)\n",
    "    return Model(inp, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As loss function we use `SparseCategoricalCrossentropy`, as we are dealing with class probabilities in the output. As optimizer we use `Adam`, which is a good general purpose optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=Adam(learning_rate=0.005))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a callback that is used to show the loss value (for training and validation) during the optimization, as well as plots the classification landscape for the model. Just run this cell, no need to understand it :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class MyCallback(Callback):\n",
    "    def __init__(self, X, Y, Xv, Yv, xlim_test =(-1.5,1.5), n_interval = 5, smooth = .4, yscale = \"log\"):\n",
    "        self._n_interval = n_interval\n",
    "        self._logs = defaultdict(list)\n",
    "        self._logs_smooth = defaultdict(list)\n",
    "        self._weights = []\n",
    "        self._x = np.linspace(*xlim_test,60)\n",
    "        self._X_test = np.stack(np.meshgrid(self._x,self._x,indexing ='ij'), axis=-1).reshape(-1,2)\n",
    "        self._X_train = X\n",
    "        self._Y_train = Y\n",
    "        self._X_val = Xv\n",
    "        self._Y_val = Yv\n",
    "        self._axs = None\n",
    "        self._yscale = yscale\n",
    "        self._smooth = smooth\n",
    "        super(MyCallback,self).__init__()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "\n",
    "        for k,v in logs.items():\n",
    "            self._logs[k].append(v)\n",
    "            if epoch==0:\n",
    "                self._logs_smooth[k].append(v)\n",
    "            else:\n",
    "                self._logs_smooth[k].append((1-self._smooth)*v+self._smooth*self._logs_smooth[k][-1])\n",
    "            \n",
    "            \n",
    "        ws = np.concatenate([w.flatten() for w in self.model.get_weights()])\n",
    "        self._weights.append(ws)\n",
    "        self.eval_and_plot(epoch) \n",
    "\n",
    "    def eval_and_plot(self, epoch=0):\n",
    "        Y_pred_test  = self.model.predict(self._X_test)\n",
    "        Y_pred_train = self.model.predict(self._X_train)\n",
    "\n",
    "        # plot every self._n_interval epoch \n",
    "        if (epoch % self._n_interval) ==0:\n",
    "\n",
    "            _, self.axs = plt.subplots(1, 2, figsize=(12,4))\n",
    "            self.axs = self.axs.flatten()    \n",
    "\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            for i,k in enumerate(self._logs.keys()):\n",
    "                self.axs[0].plot(np.arange(epoch+1),self._logs[k], color = \"C%s\"%i, alpha = .2)\n",
    "                self.axs[0].plot(np.arange(epoch+1),self._logs_smooth[k], color = \"C%s\"%i,label = k)\n",
    "                \n",
    "            self.axs[0].legend()\n",
    "            self.axs[0].set_yscale(self._yscale)\n",
    "            \n",
    "            \n",
    "            self.axs[1].contourf(self._x, self._x, Y_pred_test[...,1].reshape((len(self._x),len(self._x))).T, \n",
    "                                 100, vmin=0, vmax=1, cmap = cmap, alpha=.5)\n",
    "            \n",
    "            self.axs[1].scatter(*self._X_train.T, color = colors[self._Y_train], marker='.', alpha=.2)\n",
    "            self.axs[1].scatter(*self._X_val.T, color = colors[self._Y_val], marker='^', alpha=.6)\n",
    "            \n",
    "            self.axs[1].set_ylim(-4,4)                                 \n",
    "            self.axs[1].set_xlim(-5,5)                                             \n",
    "\n",
    "            plt.show()\n",
    "cb = MyCallback(X,Y, Xv, Yv, xlim_test = (-5,5), n_interval = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training/optimization\n",
    "\n",
    "Now we train the model, i.e. we iterate over random subsets (of size `batch_size`) of the training data, compute the loss function and adjust the model weights according to the gradients. This is repeated for `epoch` number of steps. The callback we have just defined will show you how the training/valdiation loss, as well as the classification boundary of the model change over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, batch_size=4,\n",
    "          validation_data=(Xv, Yv),\n",
    "          callbacks=[cb],\n",
    "          epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pure Python3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
